# python -m venv venv
# venv\Scripts\activate
# pip install streamlit pandas nltk scikit-learn pyarabic arabic-reshaper python-bidi

import streamlit as st
import pandas as pd
import nltk
import re
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from pyarabic.araby import strip_diacritics, strip_tatweel, normalize_hamza
from nltk.corpus import stopwords
from nltk.stem.isri import ISRIStemmer
import string

# ØªØ­Ù…ÙŠÙ„ stopwords
nltk.download('stopwords')
arabic_stopwords = set(stopwords.words('arabic'))
arabic_stopwords.update(['ÙÙŠ', 'Ø¹Ù„Ù‰', 'Ø¥Ù„Ù‰', 'Ù…Ù†', 'Ù‡Ùˆ','Ù„Ø§'])

# ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ
def clean_text(text):
    text = strip_diacritics(text)
    text = re.sub("[Ø£Ø¥Ø¢Ø§]", "Ø§", text)
    text = strip_tatweel(text)
    arabic_punctuations = '''`Ã·Ã—Ø›<>_()*&^%][Ù€ØŒ/:".,'{}~Â¦+|!"â€¦"â€“Ù€'''
    english = string.punctuation
    all_p = set(arabic_punctuations + english)
    text = "".join([char if char not in all_p else " " for char in text])
    return text

# tokenization Ø¨Ø¯ÙˆÙ† camel-tools
def tokenize(text):
    text = re.sub(r"[^\u0600-\u06FF\s]", " ", text)
    return text.split()

# stemming
stemmer = ISRIStemmer()
def stem_words(tokens):
    return [stemmer.stem(t) for t in tokens]

# Ø­Ø³Ø§Ø¨ TF
def compute_tf(processed_sentences):
    tf_data = []
    all_terms = sorted(list(set([w for sent in processed_sentences for w in sent])))

    for i, tokens in enumerate(processed_sentences, 1):
        counts = Counter(tokens)
        total = len(tokens)
        row = {"Document": f"Doc{i}"}
        for term in all_terms:
            row[term] = round(counts[term] / total, 4) if total > 0 else 0
        tf_data.append(row)

    return pd.DataFrame(tf_data)

# Streamlit UI
st.title("ğŸ“ Arabic TF-IDF Analyzer)")

st.write("Ø­Ù„ ÙƒØ§Ù…Ù„ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©: ØªÙ†Ø¸ÙŠÙØŒ ØªØ¬Ø²Ø¦Ø©ØŒ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø­Ø°ÙˆÙØ©ØŒ StemmingØŒ TFØŒ TF-IDF")

input_option = st.radio("Ø§Ø®ØªØ± Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„:", ["Ø§ÙƒØªØ¨ Ù†ØµÙˆØµ", "Ø±ÙØ¹ CSV"])

sentences = []

if input_option == "Ø§ÙƒØªØ¨ Ù†ØµÙˆØµ":
    text_input = st.text_area("Ø§ÙƒØªØ¨ ÙƒÙ„ Ø¬Ù…Ù„Ø© ÙÙŠ Ø³Ø·Ø±:", height=200)
    if text_input:
        sentences = [s.strip() for s in text_input.split("\n") if s.strip()]

elif input_option == "Ø±ÙØ¹ CSV":
    file = st.file_uploader("Upload CSV file", type=["csv"])
    if file is not None:
        df = pd.read_csv(file)
        st.write("Ù…Ø¹Ø§ÙŠÙ†Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:")
        st.dataframe(df.head())
        column = st.selectbox("Ø§Ø®ØªØ± Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø°ÙŠ ÙŠØ­ØªÙˆÙŠ Ø§Ù„Ø¬Ù…Ù„:", df.columns)
        sentences = df[column].astype(str).tolist()

if sentences:
    st.subheader("Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ø£ØµÙ„ÙŠØ©")
    st.write(sentences)

    # 1- ØªÙ†Ø¸ÙŠÙ
    cleaned = [clean_text(s) for s in sentences]
    st.subheader("1ï¸âƒ£ Ø§Ù„Ø¬Ù…Ù„ Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ")
    st.write(cleaned)

    # 2- Tokenization
    tokenized = [tokenize(s) for s in cleaned]
    st.subheader("2ï¸âƒ£ Tokenization")
    st.write(tokenized)

    # 3- Ø¥Ø²Ø§Ù„Ø© Stopwords
    filtered = [[w for w in tokens if w not in arabic_stopwords and len(w) > 1] for tokens in tokenized]
    st.subheader("3ï¸âƒ£ Ø¨Ø¹Ø¯ Ø¥Ø²Ø§Ù„Ø© StopWords")
    st.write(filtered)

    # 4- Stemming
    stemmed = [stem_words(tokens) for tokens in filtered]
    st.subheader("4ï¸âƒ£ Ø¨Ø¹Ø¯ Stemming")
    st.write(stemmed)

    # 5- TF
    tf_df = compute_tf(stemmed)
    st.subheader("5ï¸âƒ£ TF â€” Term Frequency")
    st.dataframe(tf_df)

    # 6- TF-IDF
    texts_joined = [" ".join(tokens) for tokens in stemmed]
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(texts_joined)

    tfidf_df = pd.DataFrame(
        tfidf_matrix.toarray(),
        columns=sorted(vectorizer.get_feature_names_out()),
        index=[f"Doc{i}" for i in range(1, len(sentences) + 1)]
    )

    st.subheader("6ï¸âƒ£ TF-IDF")
    st.dataframe(tfidf_df)

    # 7- Top words
    st.subheader("ğŸ” Ø£Ù‡Ù… 5 ÙƒÙ„Ù…Ø§Øª ÙÙŠ ÙƒÙ„ ÙˆØ«ÙŠÙ‚Ø©")
    for doc in tfidf_df.index:
        st.write(f"### {doc}")
        st.write(tfidf_df.loc[doc].nlargest(5))